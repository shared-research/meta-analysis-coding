---
title: "The Cognitive Effects of Computational Thinking: A Systematic Review and Meta-Analytic Study"
subtitle: "Supplementary materials"
author:
    - Chiara Montuori^[Department of Developmental and Social Psychology, chiara.montuori@phd.unipd.it]
    - Filippo Gambarota^[Department of Developmental and Social Psychology, filippo.gambarota@gmail.com]
    - Gianmarco Altoè^[Department of Developmental and Social Psychology, gianmarco.altoe@unipd.it]
    - Barbara Arfè^[Department of Developmental and Social Psychology, barbara.arfe@unipd.it]
bibliography: ../files/references.bib
csl: ../files/apa7.csl
header-includes:
    - \usepackage{fullpage}
    - \usepackage{pdflscape}
    - \usepackage{afterpage}
output:
    bookdown::pdf_document2:
        keep_tex: true
        latex_engine: lualatex
    bookdown::html_document2:
        keep_md: yes
        toc: true
        toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center",
                      fig.retina = 2,
                      fig.width = 9,
                      fig.asp = 0.618,
                      warning = FALSE,
                      message = FALSE)
```

```{r setup-dev, eval=knitr::is_html_output(), include=FALSE}
knitr::opts_chunk$set(dev = c("svg", "png"))
```

```{r packages, include=FALSE}
devtools::load_all()
library(tidyverse)
library(here)
library(flextable)
```

```{r functions, include = FALSE}
ft_to_page <- function(ft, pgwidth = 6){
  ft_out <- ft %>% autofit()
  ft_out <- width(ft_out, width = dim(ft_out)$widths*pgwidth /(flextable_dim(ft_out)$widths))
  return(ft_out)
}

# https://stackoverflow.com/a/46526740/9032257
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

```{r loading-data, include=FALSE}
dat_meta <- read_rds(here("objects", "dat_meta.rds"))
dat_meta_post <- read_rds(here("objects", "dat_meta_post.rds"))
paper_objects <- read_rds(here("objects", "paper_objects.rds"))
```

```{r loading-objects, include=FALSE}
dat_sens <- read_rds(here("objects", "dat_sens.rds"))
df_metadata <- read_rds(here("objects", "df_metadata.rds"))
```

```{r rmd-utils, include=FALSE}
rmd_format <- get_rmd_format()
```

\pagebreak

# Introduction

This document explains in detail the statistical approach of the main paper. All the analyses are conducted with R [@r-lang] and all scripts are available on the [Open Science Framework repository](https://osf.io/uvbcd/).

- Section \@ref(dataset) presents the full dataset description with all extracted variables. Despite only a subset of these variables being used to compute the meta-analysis model, we reported all extracted study-level information.
- Section \@ref(effectsize) describes the computation of the effect size measure and the sampling variance.
- Section \@ref(models) describes the main meta-analysis model presented in the paper together with modeling alternatives
- Section \@ref(correlations) describes the correlations imputation that is necessary to compute the meta-analysis model
- Section \@ref(sens) presents the sensitivity analysis for different correlations values and different meta-analytic models 

## Meta-analysis

The meta-analysis is the statistical procedure to combine information from already conducted studies. The core aspect of a meta-analysis is weighting each included effect according to the amount of information (i.e., precision) that the study provides. In order to compute a meta-analysis model we need:

- The effect size measure
- The effect size sampling variance
- The meta-analysis model

## Papers

Almost all published papers reported enough information to calculate the effect size. Di Lieto and colleagues [-@Di_Lieto2020-tc] provided us with raw data to calculate all relevant parameters. They use a design with three time points (T0 = pre, T1 = post, and T2 = follow-up) for both the experimental and control group. Furthermore, they measured several different outcome measures. To include the paper in our meta-analysis we performed these processing steps:

- We selected only the T0 and T1 to have a single pre-post measure
- We included all participants that have both time points (T0 and T1) in at least one outcome measure. For example, if a child has T0 and T1 for the outcome $x$ but only T0 for the outcome $y$ we keep the child in the analysis. This creates a situation where different outcomes have different sample sizes but maximize the amount of available information instead of including children with all outcomes.

# Dataset description {#dataset}

The main dataset can be found on the online OSF repository under `data/raw/meta_table_cleaned.csv`. The following list describes the meaning of each column: 

```{r, echo = FALSE}
df_metadata <- df_metadata %>% 
    drop_na() %>% 
    filter(description != "no") %>% 
    mutate(knitr = sprintf("- **%s**: %s \n", colname, description))
```

```{r, results='asis', echo = FALSE}
cat(df_metadata$knitr)
```

## Pre-processing steps

The raw dataset is available on the online [Open Science Framework repository](https://osf.io/uvbcd/). We performed these minimal pre-processing steps:

- renaming relevant columns
- re-coding the `outcome` variable into the `outcome2` variable (see the main paper for the rationale)
- separating the Arfè et al. (2019) paper into two separate papers (see Section \@ref(arfe2019))
- converting outcomes names into English

### Arfè et al. (2019) {#arfe2019}

Arfè et al. (2019) is the only paper that contains multiple independent (i.e., with different participants) sub-studies creating a multilevel structure. To reduce the data complexity we decided to consider these studies as two independent papers, creating Arfè et al. (2019a) and Arfè et al. (2019b).

## Multiple effects for the same outcome{#aggregation}

We decided to transform the `outcome` into the `outcome2` variable to have a smaller set of outcomes according to the underlying psychological construct. For example, if the test $x$ and the test $y$ are *Working Memory* measures we created a unique *Working Memory* variable.
This reduces the dataset complexity but creates a situation where papers have multiple effects belonging to the same outcome. We decided to aggregate multiple effects of the same `outcome2` using the approach suggested by Borenstein et al. [-@Borenstein2009-mo, pp.225-233] implemented using the `metafor::aggregate.escalc()` function (see https://wviechtb.github.io/metafor/reference/aggregate.escalc.html). 
To note, we use a slightly different approach compared to Borenstein et al.[-@Borenstein2009-mo, pp.225-233] computing an *inverse-variance weighted average* instead of an *un-weighted average*. Essentially, the weighted average combines multiple effect sizes taking into account the precision (i.e., the inverse of the variance).

We used the `metafor::aggregate.escalc()` as follows:

```{r, eval = FALSE, size = "small"}
aggregate_effects <- function(data, rho, split_by = paper, weighted = TRUE){
    split_by <- rlang::enexpr(split_by)
    data <- metafor::escalc(yi = eff_size, vi = eff_size_var, 
                   var.names = c("eff_size", "eff_size_var"),
                   data = data)
    dat_split <- split(data, pull(data, !!split_by))
    dat_split <- purrr::map_dfr(dat_split, function(x) {
        metafor::aggregate.escalc(x, 
                                  cluster = outcome2, 
                                  rho = rho, 
                                  weighted = weighted)
    })
    bind_rows(dat_split) %>% tibble()
}
```

This is a wrapper of the `metafor::aggregate.escalc()` function that splits the dataset according to the `paper` and then aggregates within each `outcome2`.

# Effect size computation {#effectsize}

All included studies used a Pretest-Posttest-Control Group (PPC) design where the treated group is compared with an age-matched control group before and after a certain treatment. Morris [-@Morris2008-pe] described different effect size indexes for the PPC design. We decided to use the $d_{ppc2}$ index because it provides an unbiased estimation of the true effect size and the sampling variance can be analytically computed. The $d_{ppc2}$ is calculated as indicated in Equation \@ref(eq:dppc).

\begin{equation} 
  d_{ppc2} = c_p \frac{(M_{T,post} - M_{T,pre}) - (M_{C,post} - M_{C,pre})}{SD_{pooled,pre}}
  (\#eq:dppc)
\end{equation}

The numerator describes the actual mean difference between the treated and control groups of the respective *pre-post* scores. The difference is standardized using only a pooled standard deviation of pre-test scores. The variance is calculated in Equation \@ref(eq:dppc-sd):

\begin{equation} 
  \sigma^2(d_{ppc2}) = c^2_p(1-\rho)(\frac{n_t+n_c}{n_t n_c})(\frac{n_t+n_c-2}{n_t+n_c-4})(\frac{1 + \Delta^2}{2(1-\rho)(\frac{n_t+n_c}{n_t n_c})})
  (\#eq:dppc-sd)
\end{equation}

Both the $d_{ppc2}$ and the variance are adjusted using a small sample correction factor $c_p$ calculated in Equation \@ref(eq:cp)

\begin{equation} 
  c_p = 1 - \frac{3}{4(2n_t + 2n_c - 4) - 1}
  (\#eq:cp)
\end{equation}

Positive effect size values mean that the treatment increased the experimental group's performance. However, for some measures, higher values correspond to worse performance (e.g., the number of errors). In this case, we computed the effect size as described in the previous section and then we flipped the sign obtaining the same interpretation regardless of the original measure.

For the actual computation we wrote two functions following the documentation of the `metafor` package: (http://www.metafor-project.org/doku.php/analyses:morris2008), `get_dppc2()`:

```{r, eval = FALSE}
get_dppc2 <- function(mt_pre, mt_post, mc_pre, mc_post,
                      st_pre, st_post, sc_pre, sc_post,
                      nt, nc){
    
    mdiff <- (mt_post - mt_pre) - (mc_post - mc_pre)
    poolvar <- sqrt(((((nt - 1) * st_pre^2) + ((nc - 1) * sc_pre^2)) / (nt + nc - 2)))
    cp <- metafor:::.cmicalc(nt + nc - 2)
    
    dppc2 <- (mdiff / poolvar) * cp
    
    return(dppc2)
    
}
```

and `get_dppc2_var()`:

```{r, eval = FALSE}

get_dppc2_var <- function(dppc2, nt, nc, rho){
    
    dppc2_var <- 2*(1-rho) * (1/nt + 1/nc) + dppc2^2 / (2*(nt + nc))
    
    return(dppc2_var)
    
}
```

# Meta-analysis models {#models}

## Univariate vs multivariate model {#uni-vs-multi}

When multiple variables are collected from the same pool of participants (e.g., multiple outcomes studies) and/or there are multiple experiments within the same paper, the effect size cannot be considered independent [@Cheung2019-po; @Cheung2014-fg]. Instead of pooling multiple effects into a single measure or keeping only a single effect, we computed a multivariate meta-analysis model taking into account the dependency (i.e., the correlation) between multiple effect sizes calculated on the same pool of participants. To compute the multivariate model we need to include or impute the variance-covariance matrix that describes how multiple effects correlate within each study. The section \@ref(correlations) explains our imputation approach.

## Random-effect vs fixed-effect model

Another important choice is between the fixed-effect and the random-effect model. Under the fixed-effect model, we make the assumption of a single *true* effect size that is estimated by a sample of studies. For this reason, the variability observed in the empirical meta-analysis is only caused by the sampling error.

On the other side, the *random-effect* model assumes a *distribution* of *true* effects. The model estimate the average effect (the *mean* of the distribution) and the between-study heterogeneity $\tau^2$ (i.e., the *variance* of the distribution). The critical assumption is that the observed heterogeneity is composed of sampling variability (as the *fixed-effect* model) and *true* between-study variability. The latter can be caused by different experimental designs, participants' features, or other study-level variables and can be explained using moderators (i.e., meta-regression).

The goal of the *random-effect* model is to generalize the meta-analytic findings at the population level while the *fixed-effect* model is focused on a specific pool of studies. 

We decided to use a *fixed-effect* model for several reasons.  Firstly, one reason to estimate $\tau^2$ (i.e., the focus of the *random-effect* model) is to explain the observed heterogeneity with study-level variables. Given our limited pool of studies, we did not include a meta-regression analysis. Furthermore, Under the *random-effect* framework, the multivariate model estimates the average effect and the variability ($\tau^2$) for each included outcome increasing the model complexity compared to the univariate counterpart. Crucially, with a limited number of studies the $\tau^2$ estimation can be strongly biased [@Veroniki2016-nw] influencing also the pooled effect estimation [@Borenstein2009-mo, pp.73-75] especially in terms of the standard error. Finally, the *fixed-effect* is less complex in terms of model parameters because we need to estimate only the average effect of each included outcome, taking into account the statistical dependency (see \@ref(uni-vs-multi)).

## Modelling functions

We tested each model parameter (i.e., average effect for a specific outcome) using the Wald *z-test* with $\alpha = 0.05$.  
For the *univariate fixed-effect* model we use the `metafor::rma()` function:

```{r, eval = FALSE}
fit_uni_fixed <- function(data){
    rma(yi = eff_size, 
        vi = eff_size_var,
        method = "FE",
        data = data)
}
```

For the *univariate random-effect* model we use the `metafor::rma()` function and the REML estimator for $\tau^2$:

```{r eval = FALSE}
fit_uni_random <- function(data){
    rma(yi = eff_size, 
        vi = eff_size_var,
        method = "REML",
        data = data)
}
```

For the *multivariate fixed-effect* model we use the `metafor::rma.mv()` function:

```{r eval = FALSE}
fit_multi_fixed <- function(data, cov_matrix){
    rma.mv(
        yi = eff_size,
        V = cov_matrix,
        mods = ~ 0 + outcome2,
        data = data)
}
```

For the *multivariate random-effect* model we use the `metafor::rma.mv()` function:

```{r eval = FALSE}
fit_multi_random <- function(data, cov_matrix, struct = "UN"){
    rma.mv(
        yi = eff_size,
        V = cov_matrix,
        mods = ~ 0 + outcome2,
        random = ~ outcome2|paper_id,
        # the variance-covariance matrix structure 
        # see https://wviechtb.github.io/metafor/reference/rma.mv.html
        struct = struct,
        data = data)
}
```

In both the multivariate cases we used the `outcome2` variable as a moderator with a *cell-mean parametrization* [@Schad2020-ht] (i.e., removing the intercept `0 + outcome2`). In this way model parameters and statistical tests correspond directly to the average effect for each outcome.  As explained in the previous section, the random-effect model estimate also a $\tau^2$ for each outcome (written as `random = ~ outcome2|paper_id`, see https://www.metafor-project.org/doku.php/analyses:berkey1995).

### Variance-covariance matrix

For the *multivariate* models (both fixed and random) is necessary to include a variance-covariance matrix. Essentially, each study has $n$ different outcomes and we need a $n \times n$ variance-covariance matrices where the diagonal is the $d_{pcc2}$ variance for a specific outcome and off-diagonal elements are the covariances between pairs of outcomes.

Combining all study-level matrices we obtain a full block-variance-covariance matrix to use within the `rma.mv()` function. The `metafor::vcalc()` (see https://wviechtb.github.io/metafor/reference/vcalc.html) function allows to create the full matrix specifying the assumed correlation and a clustering variable:

```{r, eval = FALSE}
get_block_cov_matrix <- function(data, rho){
    vcalc(eff_size_var, cluster = paper_id, obs = effect_id, data = data, rho = rho)
}
```

# Correlations imputation{#correlations}

As explained in previous sections, we decided to use a *fixed-effect multivariate* model. To compute the model we need to include 3 correlations measures:

- For the effect size computation, we need the correlation between pre-post (*pre-post correlation*) scores for both groups
- For the aggregation of multiple effects within the same paper (*aggregation correlation*) (see Section \@ref(aggregation)) we need the correlation between the effects
- For the actual multivariate model we need the full variance-covariance matrix of different outcomes. To create the matrix we need to include the correlation between outcomes within each study (*multivariate correlation*).

Correlations are rarely reported in published papers, thus we decided to impute these values and assess the impact with a multiverse-like approach [@Steegen2016-lz]. In particular, we used:

- A $\rho_{pre-post}$ of 0.5, 0.7 and 0.9
- A $\rho_{agg}$ of 0.3, 0.5, 0.7
- A $\rho_{multi}$ of 0.3, 0.5 and 0.7

# Meta-analysis table

Table \@ref(tab:metatab) describes all included papers with pre-post scores for the experimental and control groups and the computed effect size. Furthermore, the table is organized grouping the effects according to the considered outcome in order to highlight the multivariate data structure.

```{r metatab, echo=FALSE}
paper_objects$meta_table %>% 
    fontsize(size = 7, part = "all") %>% 
    ft_to_page(6) %>% 
    set_caption("Pre-post scores (M = mean, SD = standard deviation, N = sample size) for the control and experimental group. The last two columns represent the computed effect size and the variance. The effects are organized according to the outcome value and the corresponding paper.")
```

\newpage

# Multiverse analysis{#sens}

We assessed the impact of our modeling assumptions using a sensitivity analysis approach. In particular, we considered:

- 4 meta-analysis models:
    - *univariate fixed-effect*
    - *univariate random-effect*
    - *multivariate fixed-effect*
    - *multivariate random-effect*
- All correlations combinations ($3 \rho_{pre-post} \times 3 \rho_{aggregation} \times 3 \rho_{multivariate}$)

In this way, we computed `r 3*3*3*2+3*3*2` meta-analysis directly assessing the impact on the parameters estimation and p-values^[For *multivariate* models (*fixed* and *random*) we have 3x3x3 correlation combinations while for *univariate* models (*fixed* and *random*) only 3x3 ($\rho_{pre-post}$ and $\rho_{agg}$)]. All plots in the following pages depicted the sensitivity analysis for each outcome. Triangles indicate parameters with $p < 0.05$ while points indicate parameters with $p > 0.05$. Red shapes indicate the correlations/model combination included in the paper. Clearly, *random effect models* (both *univariate* and *multivariate*) are associated with wider confidence intervals (i.e., less precise estimation) compared to *fixed effect models*.

`r if(rmd_format == "pdf") {"\\begin{landscape}"}`

```{r, echo=FALSE}
# here we removed some redundant elements from the ggplot object 
elem_to_remove <- c(
    sprintf("axis-b-%d-4", c(2,4,6,8)),
    sprintf("panel-%s-%s", c(3,4,1,4,1,2,1,2,3,4,3,2), c(5,5,6,7,8,8,7,7,7,9,9,9))
)
```

```{r, echo = FALSE, cognitive-flexibility-acc-plot}
dat_sens$plot_sens_estimate[[1]] %>%  
    remove_element_ggplot(elem_to_remove)
```

```{r, echo = FALSE, inhibition-accuracy-plot}
dat_sens$plot_sens_estimate[[2]] %>%  
    remove_element_ggplot(elem_to_remove)
```

```{r, echo = FALSE, planning-accuracy-plot}
dat_sens$plot_sens_estimate[[3]] %>%  
    remove_element_ggplot(elem_to_remove)
```

```{r, echo = FALSE, problem-solving-plot}
dat_sens$plot_sens_estimate[[4]] %>%  
    remove_element_ggplot(elem_to_remove)
```

```{r, echo = FALSE, working-memory-accuracy-plot}
dat_sens$plot_sens_estimate[[5]] %>%  
    remove_element_ggplot(elem_to_remove)
```

`r if(rmd_format == "pdf") {"\\end{landscape}"}`

# References
